<?xml version="1.0" encoding="UTF-8"?>
<html xmlns:xlink="http://www.w3.org/1999/xlink">
    <style type="text/css">
          .alignmentCenter{
            text-align: center;
              }
           .setWidth{
             width: 50%;
           }
          }
        </style>
    <body>
        <h1 class="alignmentCenter">Satisfiability Modulo Theories Competition (SMT-COMP)</h1>
        <h2>Satisfiability Modulo Theories Solvers, 2012-06-30 to 2012-07-01, Manchester, UK, IJCAR, SMT workshop</h2>
        <h3>
        The SMT workshop will include a block of time to present the competitors and results of the SMTCOMP competition.
        The Satisfiability Modulo Theories Competition (SMT-COMP) arose from the SMT-LIB (``Satisfiability Modulo Theories Library'') 
        initiative to spur adoption of the common, community-designed SMT-LIB formats, and to spark further advances in SMT, especially for verification.
      </h3>
        <h3>Rules</h3>
        <b>Rules Information: </b>
        <br/>Official List of rules<br/>
        <b>Links-</b>
        <br/>
        <li>
            <a href="http://smtcomp.sourceforge.net/2012/rules12.pdf">Rule List</a>
        </li>
        <br/>
        <br/>
        <b>Submission Requirement: </b>
        <br/>
          Submission this year will be through the SMT-Exec service, an execution service for SMT solvers.
          You are encouraged to upload early and often to test your solver on the competition infrastructure.
          SMT-Exec allows you to keep such submissions private, and when you have completed your competition solver,
          you can upload the final version of your solver for competition, or mark one of your previous uploads as your competition submission.
        <br/>
        <br/>
        <b>Benchmarks: </b>
        <br/>Benchmark Information<br/>
        <b>Links-</b>
        <br/>
        <li>
            <a href="http://smtcomp.sourceforge.net/2012/call12.txt">Call for benchmarks</a>
        </li>
        <li>
            <a href="http://smtcomp.sourceforge.net/2012/application.shtml">Sample Benchmarks</a>
        </li>
        <br/>
        <br/>
        <h3>Supporting Documents</h3>
        <table border="1">
            <tr bgcolor="#9acd32">
                <th>Document Name</th>
                <th>Description</th>
                <th>Link</th>
            </tr>
            <tr>
                <td class="setWidth">Disussion List</td>
                <td>-</td>
                <td>
                    <table>
                        <tr>
                            <td>
                                <a href="http://cs.nyu.edu/mailman/listinfo/smt-comp">Discussion list for the Satisfiability Modulo Theories Competition</a>
                            </td>
                        </tr>
                    </table>
                </td>
            </tr>
            <tr>
                <td class="setWidth">SMP-COMP Papers</td>
                <td>-</td>
                <td>
                    <table>
                        <tr>
                            <td>
                                <a href="http://smtcomp.sourceforge.net/2012/docs.shtml">SMT-COMP Papers</a>
                            </td>
                        </tr>
                    </table>
                </td>
            </tr>
            <tr>
                <td class="setWidth">Final Competition Report and some final system descriptions</td>
                <td>-</td>
                <td>
                    <table>
                        <tr>
                            <td>
                                <a href="http://smtcomp.sourceforge.net/2012/reports/SMTCOMP2012.pdf">The 2012 SMT Competition</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <a href="http://smtcomp.sourceforge.net/2012/reports">2012 Reports</a>
                            </td>
                        </tr>
                    </table>
                </td>
            </tr>
        </table>
        <h3>Participants-</h3>The details are given in the following link<br/>
        <li>
            <a href="http://www.smtexec.org/exec/competitors2012.php">
        Competitors: SMT-COMP 2012
      </a>
        </li>
        <h3>Benchmarks</h3>
        <b>Application track Benchmarks</b>
        <br/>Description:- 
          Benchmarks and tools are subject to change until their respective freeze dates, as documented in the SMT-COMP rules<br/>Format:- 
          Each tarball contains one or two status files for each benchmark:
            BENCHMARK.results.txt is always present, and
            BENCHMARK.results_untrusted.txt might be present as well
          These files contain one status per line, corresponding to the series of (check-sat) commands in the benchmarks.
          Each status line (where different from "unknown") has been determined by running at least 
          3 different SMT solvers on the set of instances resulted from "unfolding" each incremental benchmark using the scrambler. 
          For *.results.txt, all the results different from "unknown" have been reported by at least 2 different solvers, whereas all 
          the status lines generated by a single solver only (because e.g. the others timed out) have been replaced with "unknown". 
          For *.results_untrusted.txt, the single-solver answer is also included. However, they are marked with an "# untrusted" comment.
        <br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/application.shtml">
         Application track Benchmarks
        </a>
        </li>
        <br/>
        <b>Unsat core benchmarks</b>
        <br/>Description:- 
          The unsat core benchmarks are the subset of the benchmarks from the indicated logics that are unsatisfiable; the benchmarks themselves have been modified to include names for the assertions. Per the SMTLIB standard, the benchmarks are allowed to contain a mix of named and unmaned formulae, though ordinarily,
          all top-level formulae will have names. The names may be scrambled by the benchmark scrambler. 
        <br/>Format:- 
          Each tarball contains one or two status files for each benchmark:
          BENCHMARK.results.txt is always present, and
          BENCHMARK.results_untrusted.txt might be present as well
          These files contain one status per line, corresponding to the series of (check-sat) commands in the benchmarks.
          Each status line (where different from "unknown") has been determined by running at least
          3 different SMT solvers on the set of instances resulted from "unfolding" each incremental benchmark using the scrambler.
          For *.results.txt, all the results different from "unknown" have been reported by at least 2 different solvers, whereas all
          the status lines generated by a single solver only (because e.g. the others timed out) have been replaced with "unknown".
          For *.results_untrusted.txt, the single-solver answer is also included. However, they are marked with an "# untrusted" comment.
        <br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/application.shtml">
          Unsat core benchmarks
        </a>
        </li>
        <br/>
        <h3>Expected Solution</h3>
        <b>Execution Enviornment-</b>
        <br/>Description:- 
          The competition will run on a cluster of 11 machines at The University of Iowa.
          All eleven execution nodes have the same OS configuration. 
          However, please note that we may make software and configuration changes before the competition. 
          When you submit your solver for SMT-COMP through the SMT-Exec service, it is checked for compatibility with the competition 
          infrastructure on simple benchmarks supported by your solver. Note that the organizers had expected the 
          Star-Exec service to be ready for the 2012 competition; the Star-Exec service would have provided considerably 
          more resources. However, development delays have required us (as of mid-May 2012) to plan to use SMT-Exec again this year.
        <br/>Library Name: - runtime libraries from gcc-4.1.2-42.el5<br/>
        <br/>Compiler Name: - glibc-2.5-24.el5_2.2<br/>Processor Name: - AMD Opteron 250s<br/>Processor Memory: - 
              1 MB cache and 4 GB main memory
            <br/>Processor Description: - Out of 11 cluster machines, 9 are of this type. 2.4 GHz, configured for single core, 64-bit processing
            <br/>Processor Name: - Intel Xeon E5540s<br/>Processor Memory: - 
              8 MB cache and 12 GB main memory
            <br/>Processor Description: - Out of 11 cluster machines, 2 are of this type. 2.53 GHz, configured for single core, 64-bit processing
            <br/>OS Name: - Red Hat Enterprise Linux Client release 5.2 (Tikanga)<br/>OS Version: - Linux, kernel version 2.6.18-128.2.1.el5<br/>
        <b>Deadlines-</b>
        <table border="1">
            <tr bgcolor="#9acd32">
                <th>Deadline For</th>
                <th>Date</th>
            </tr>
            <tr>
                <td>Call for benchmarks and solvers, and draft rules for comment are made public</td>
                <td>2012-01-23T00:00:00</td>
            </tr>
            <tr>
                <td>Revised rules posted; new benchmarks available; call for solvers restated</td>
                <td>2012-06-02T00:00:00</td>
            </tr>
            <tr>
                <td>Competition started - results online</td>
                <td>2012-06-23T00:00:00</td>
            </tr>
            <tr>
                <td>Competition report and system descriptions posted</td>
                <td>2012-08-26T00:00:00</td>
            </tr>
        </table>
        <b>Tool Name: - </b>Benchmark scrambler<br/>
        <b>Description: - </b>This tool can be downloaded as a zip file<br/>Links:- <li>
            <a href="https://es.fbk.eu/people/griggio/smtcomp/smtcomp2012_scrambler.tar.gz">
          Download Benchmark scrambler
        </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Benchmark selection tool<br/>
        <b>Description: - </b>Python program to generate benchmark selection<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/generate_benchmark_selection_list.py">
          Benchmark selection tool
        </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Benchmark retrieval tool<br/>
        <b>Description: - </b>Gets the selected benchmarks<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/get_selected_benchmarks_paths.py">
          Benchmark retrieval tool
        </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Trace executor<br/>
        <b>Description: - </b>Download Trace Executor<br/>Links:- <li>
            <a href="https://es.fbk.eu/people/griggio/smtcomp/smtcomp2012_trace_executor.tar.gz">
            Trace executor
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Benchmark selection<br/>
        <b>Description: - </b>Benchmark Selection Details<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/bench_selection.shtml">
            Benchmark selection
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>TreeLimitedRun wrapper (borrowed from CASC and modified)<br/>
        <b>Description: - </b>Program to watch CPU usage of a process<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/TreeLimitedRun.c">
            TreeLimitedRun wrapper
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>SMT-LIB benchmarks<br/>
        <b>Description: - </b>Information about SMT-LIB benchmarks<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/smtlib2012-benchs.txt.bz2">
            SMT-LIB benchmarks
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Eligible Benchmarks<br/>
        <b>Description: - </b>Benchmarks eligible for the 2012 competition<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/smtcomp2012-eligible.txt.bz2">
            Download Benchmarks eligible for the 2012 competition
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>Unsat core track<br/>
        <b>Description: - </b>Script to run the unsat core track<br/>Links:- <li>
            <a href="http://smtcomp.sourceforge.net/2012/run_unsat_core_track.sh">
            Download Script  
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>SMT-LIBv2 parser<br/>
        <b>Description: - </b>Script to write SMT-LIBv2 parser<br/>Links:- <li>
            <a href="https://es.fbk.eu/people/griggio/misc/smtlib2parser.html">
            Download Script  
          </a>
        </li>
        <br/>
        <b>Tool Name: - </b>SMT-LIBv2 tutorial<br/>
        <b>Description: - </b>SMT-LIBv2 tutorial and resources (thanks to GrammaTech's David Cok)<br/>Links:- <li>
            <a href="http://www.grammatech.com/resources/smt/">
            Download Link  
          </a>
        </li>
        <br/>
        <h3>Contacts</h3>
        <li>
            <b>Roberto Bruttomesso - Organizer</b>
        </li>
        <a href="http://www.inf.usi.ch/postdoc/bruttomesso/">
            Roberto Bruttomesso Web Link
          </a>
        <br/>
        <br/>
        <li>
            <b>David Cok - Organizer</b>
        </li>
        <br/>
        <li>
            <b> Alberto Griggio - Organizer</b>
        </li>
        <a href="https://es.fbk.eu/people/griggio/">
            Alberto Griggio Web Link
          </a>
        <br/>
        <br/>
        <li>
            <b>Acknowledgement</b>
        </li>
        <a href="http://smtcomp.sourceforge.net/2012/acknowledgments.shtml">
            Acknowledgement Web Link
          </a>
        <br/>
        <br/>
    </body>
</html>
